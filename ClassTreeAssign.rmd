---
output:
  word_document: default
  html_document: default
---
# BAN 502 - Classification Trees Assignment

## Khayrayyah Haamid-Day

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(RColorBrewer)

heart <- read_csv("heart_disease-1.csv")

```

```{r}
heart = heart %>% mutate(Sex = as_factor(Sex)) %>%
  mutate(ChestPainType = as_factor(ChestPainType)) %>%
  mutate(RestingECG = as_factor(RestingECG)) %>%
  mutate(ExerciseAngina = as_factor(ExerciseAngina)) %>%
  mutate(HeartDisease = as_factor(HeartDisease)) %>%
  mutate(HeartDisease = fct_recode(HeartDisease, "No" = "0", "Yes" = "1"))
```

```{r}
#Task 1
set.seed(12345)

heart_split = initial_split(heart, prop = 0.7, strata = HeartDisease) 
train = training(heart_split) 
test = testing(heart_split)
```

```{r}
#Task 2
heart_recipe = recipe(HeartDisease ~., train) %>%
  step_dummy(all_nominal(),-all_outcomes())

tree_model = decision_tree() %>% 
  set_engine("rpart", model = TRUE) %>% 
  set_mode("classification")

heart_wflow = 
  workflow() %>% 
  add_model(tree_model) %>% 
  add_recipe(heart_recipe)

heart_fit = fit(heart_wflow, train)

tree = heart_fit %>% 
  extract_fit_parsnip() %>% 
  pluck("fit")


fancyRpartPlot(tree)
```

```{r}
#Task 3
heart_fit$fit$fit$fit$cptable
```
CP Value of .010 is the most optimal.

```{r}
#Task 4 - K-fold
set.seed(123)
folds = vfold_cv(train, v = 5)

#Tuning Grid
heart_recipe = recipe(HeartDisease ~., train) %>%
  step_dummy(all_nominal(),-all_outcomes())

tree_model = decision_tree(cost_complexity = tune()) %>% 
  set_engine("rpart", model = TRUE) %>% #don't forget the model = TRUE flag
  set_mode("classification")

tree_grid = grid_regular(cost_complexity(),
                          levels = 25) #try 25 sensible values for cp

heart_wflow = 
  workflow() %>% 
  add_model(tree_model) %>% 
  add_recipe(heart_recipe)

tree_res = 
  heart_wflow %>% 
  tune_grid(
    resamples = folds,
    grid = tree_grid
    )

tree_res
```

```{r}
#Model Performance
tree_res %>%
  collect_metrics() %>%
  ggplot(aes(cost_complexity, mean)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 
```

```{r}
#Task 5
best_tree = tree_res %>%
  select_best("accuracy")

best_tree
```

CP value of .042 yields the optimal accuracy value.

```{r}
#Task 6
final_wf = 
  heart_wflow %>% 
  finalize_workflow(best_tree)

final_fit = fit(final_wf, train)

tree2 = final_fit %>% 
  pull_workflow_fit() %>% 
  pluck("fit")

fancyRpartPlot(tree2, tweak = 1.5) 

```

```{r}
#Task 7
tree_pred = predict(final_fit, train, type = "class")
head(tree_pred)

confusionMatrix(tree_pred$.pred_class,train$HeartDisease,positive = "Yes")
```

The accuracy of this tree is 83% or 84%, if rounded up.

```{r}
#Task 8
blood <- read_csv("Blood.csv")

blood = blood %>% mutate(DonatedMarch = as_factor(DonatedMarch)) %>%
  mutate(DonatedMarch = fct_recode(DonatedMarch, "No" = "0", "Yes" = "1"))

summary(blood)
```

```{r}
#Task 9
set.seed(1234)

blood_split = initial_split(blood, prop = 0.7, strata = DonatedMarch) 
train2 = training(blood_split) 
test2 = testing(blood_split)

blood_recipe = recipe(DonatedMarch ~., train2) %>%
  step_dummy(all_nominal(),-all_outcomes())

tree_model2 = decision_tree() %>% 
  set_engine("rpart", model = TRUE) %>% 
  set_mode("classification")

blood_wflow = 
  workflow() %>% 
  add_model(tree_model2) %>% 
  add_recipe(blood_recipe)

blood_fit = fit(blood_wflow, train2)

tree3 = blood_fit %>% 
  extract_fit_parsnip() %>% 
  pluck("fit")

#K-fold
set.seed(1234)
folds = vfold_cv(train2, v = 5)

#Tuning Grid
blood_recipe2 = recipe(DonatedMarch ~., train2) %>%
  step_dummy(all_nominal(),-all_outcomes())

tree_model3 = decision_tree(cost_complexity = tune()) %>% 
  set_engine("rpart", model = TRUE) %>% 
  set_mode("classification")

tree_grid2 = grid_regular(cost_complexity(),
                          levels = 25) 

blood_wflow2 = 
  workflow() %>% 
  add_model(tree_model3) %>% 
  add_recipe(blood_recipe2)

tree_res2 = 
  blood_wflow2 %>% 
  tune_grid(
    resamples = folds,
    grid = tree_grid2)

tree_res2

```

```{r}
#Model Performance
tree_res2 %>%
  collect_metrics() %>%
  ggplot(aes(cost_complexity, mean)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 
```
```{r}
#Task 10
best_tree2 = tree_res2 %>%
  select_best("accuracy")
best_tree2

final_wf2 = 
  blood_wflow2 %>% 
  finalize_workflow(best_tree2)

final_fit2 = fit(final_wf2, train2)

tree4 = final_fit2 %>% 
  pull_workflow_fit() %>% 
  pluck("fit")

fancyRpartPlot(tree4, tweak = 1.5) 
```
```{r}
#Task 11
tree_pred2 = predict(final_fit2, train2, type = "class")
head(tree_pred2)

confusionMatrix(tree_pred2$.pred_class,train2$DonatedMarch,positive = "Yes")

tree_pred3 = predict(final_fit2, test2, type = "class")
head(tree_pred3)

confusionMatrix(tree_pred3$.pred_class,test2$DonatedMarch,positive = "Yes")
```
The training set has a accuracy of 81% and the testing set is 78%; tree's performance on these sets is good. 
